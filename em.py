#!/usr/bin/env python

# Copyright (C) 2020-2025 Simon Crase  simon@greenweaves.nz

# This is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.

# This software is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with GNU Emacs.  If not, see <http://www.gnu.org/licenses/>.

'''
   Get best GMM fit as described in:
   Note Set 4: The EM Algorithm for Gaussian Mixtures
   Padhraic Smyth
   https://ics.uci.edu/~smyth/courses/cs274/notes/notes2022/notes5b.pdf
'''

from argparse import ArgumentParser
from os.path import basename,join
from random import choice
from time import time
from scipy.stats import norm
from matplotlib.pyplot import figure, rcParams, show
import numpy as np

def maximize_likelihood(xs, mu=np.array(0), sigma=np.ones((1)), alpha=np.ones((1)), K=3, N=25,
                        atol=1.0e-6, n_burn_in=3):
    '''
    Maximize likelihood

    Parameters:
        xs         Data series
        mu         Starting values for means
        sigma      Starting values for standard deviations
        alpha      Starting value for proportion of data points assigned to each cluster
        K          Number of Gaussians that we are using to model data
        N          Maximum number of iterations
        atol       Tolerance: used to decide whether algorithm has converged
        n_burn_in  Number of steos before we start checking for convergence
    '''
    def get_log_likelihood(mu=np.array(0), sigma=np.ones((1)), alpha=np.ones((1))):
        '''
        Log of likelihood that xs were generated by current set of parameters

        Parameters:
            mu
            sigma
            alpha
        '''
        L = 0
        for i in range(len(xs)):
            for k in range(K):
                L += np.log(norm.pdf(xs[i], loc=mu[k], scale=sigma[k]))
        return L

    def e_step(mu=np.array(0), sigma=np.ones((1)), alpha=np.ones((1))):
        '''
           Perform E step from EM algorithm.
        '''
        w = np.empty((len(xs),K))
        for i in range(len(xs)):
            for k in range(K):
                w[i,k] = norm.pdf(xs[i],loc=mu[k],scale=sigma[k]) * alpha[k]

        row_sums = w.sum(axis=1)
        return w / row_sums[:, np.newaxis]


    def m_step(w):
        '''
        Perform M step from EM algorithm: calculate new values for alpha, mu, and sigma
        '''
        # Proportion of data points assigned to each k
        alpha = np.sum(w,axis=0)/np.sum(w)

        mu = np.empty((K))
        for k in range(K):
            mu[k] = np.dot(w[:,k],xs)
        mu /= np.sum(w,axis=0)

        sigma = np.zeros((K))
        for k in range(K):
            sigma[k] += np.dot(w[:,k], (xs[:] - mu[k])**2)
        sigma = sigma/ np.sum(w,axis=0)

        return (alpha, mu, sigma)

    # Use E-step and M- Step to update alpha, mu, and sigma as long as likelihoods keep improving

    likelihoods = [get_log_likelihood(mu=mu,sigma=sigma,alpha=alpha)]

    for i in range(N):
        if i > n_burn_in and abs(likelihoods[-1] / likelihoods[-2] - 1) < atol:
            return np.array(likelihoods), alpha, mu, sigma

        alpha, mu, sigma = m_step(e_step(mu=mu,sigma=sigma,alpha=alpha))
        likelihoods.append(get_log_likelihood(mu=mu,sigma=sigma,alpha=alpha))

def plot_data(xs, mu, sigma, ax=None):
    '''
    Plot the data and the fitted Gaussian
    '''
    def normalize(ys,n):
        '''
        Used to scale a probability density so it fits into the same y range as an empirical distribution

        Parameters:
            ys     Points specifiying probability density
            n      Empirical distribution
        '''
        return (n.max()/ ys.max()) * ys

    n, bins, _ = ax.hist(xs, bins=50, label=fr'1: Data $\mu=${np.mean(xs):.3f}, $\sigma=${np.std(xs):.3f}',color='b')
    midpoints = (bins[:-1] + bins[1:]) / 2
    density = normalize(norm.pdf(midpoints, loc=mu, scale=sigma),n)
    ax.plot(midpoints,density,color='c',label=fr'2: EM $\mu=${mu[0]:.3f}, $\sigma=${sigma[0]:.3f}')
    ax.set_title('Data compared to  EM')
    ax.legend()

def plot_Likelihoods(Likelihoods, ax=None):
    ax.set_title('Progress')
    ax.plot(Likelihoods)
    ax.set_xticks(range(1, len(Likelihoods)))
    ax.set_ylabel('log Likelihood')
    ax.set_xlabel('Iteration')


def parse_args():
    parser = ArgumentParser(__doc__)
    parser.add_argument('--N', type=int, default=5000, help='Dataset size')
    parser.add_argument('--mean', type=float, default=0.5, help='Mean for dataset')
    parser.add_argument('--sigma', type=float, default=0.5, help='Standard deviation')
    parser.add_argument('--seed', type=int, default=None, help='Seed for random number generator')
    parser.add_argument('--show', action='store_true', default=False, help='Show plots')
    parser.add_argument('--figs', default='./figs', help='Folder to store plots')
    return parser.parse_args()


if __name__ == '__main__':
    rcParams.update({
        "text.usetex": True
    })

    args = parse_args()

    rng = np.random.default_rng(args.seed)
    start = time()
    xs = rng.normal(loc=args.mean, scale=args.sigma, size=args.N)

    L, _, mu, sigma = maximize_likelihood(xs, mu=np.array([rng.choice(xs)]), sigma=2*np.ones((1)), alpha=np.ones((1)), K=1)
    fig = figure(figsize=(10, 10))
    plot_data(xs, mu, sigma, ax=fig.add_subplot(2,1,1))

    plot_Likelihoods(L, ax=fig.add_subplot(2,1,2))
    fig.savefig(join(args.figs,basename(__file__).split('.')[0]))

    print(f'N={args.N},  EM: {(time() - start):.3f} sec')
    if args.show:
        show()
