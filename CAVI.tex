%    Copyright (C) 2020-2021 Greenweaves Software Limited

%   This program is free software: you can redistribute it and/or modify
%   it under the terms of the GNU General Public License as published by
%   the Free Software Foundation, either version 3 of the License, or
%   (at your option) any later version.

%  This program is distributed in the hope that it will be useful,
%  but WITHOUT ANY WARRANTY; without even the implied warranty of
%  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%  GNU General Public License for more details.

%  You should have received a copy of the GNU General Public License
%  along with this program.  If not, see <https://www.gnu.org/licenses/>.

\documentclass[]{article}

\usepackage{amsmath,amssymb,amsfonts,float,mathrsfs,thmtools,url}
\usepackage[toc,page]{appendix}
\usepackage[toc,acronym,nonumberlist]{glossaries}

\newcommand{\ELBO[1]}        {\mathscr{L}}
\newcommand{\Expectation} {\mathbb{E}}
\newcommand{\KLD}[2]{D_{\mathrm{KL}} \left( \left. \left. #1 \right|\right| #2 \right) }
\newcommand\numberthis    {\addtocounter{equation}{1}\tag{\theequation}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}[thm]{Definition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}

%opening
\title{Coordinate Ascent Variational Inference}
\author{Simon Crase}

\makeglossaries

\newacronym{gls:CAVI}{CAVI} {Coordinate Ascent Variational Inference}
\newacronym{gls:ELBO}{ELBO} {Evidence Lower Bound}
\newacronym{gls:GMM} {GMM}  {Gaussian Mixture Model}
\newacronym{gls:VI}  {VI}   {Variational Inference}

\newglossaryentry{gls:ELBOterm}{
	name={Evidence Lower Bound},
	description={Evidence Lower Bound \eqref{eq:ELBO}, a lower bound on the probability of observing some data under a model},
	symbol={$\ELBO$}
}

\begin{document}

\maketitle

\begin{abstract}
	This document aims to fill in the gaps in derivations between published papers, such as \cite{blei2017variational}, and my code. It has been written to help my understanding. If it is helpful to anyone else it is a bonus.
\end{abstract}



\tableofcontents
\listoftables
\section{Evidence Lower Bound}

The notation used in this document is defined in Table \ref{table:notation}.

\begin{table}[H]
	\begin{center}
		\caption{Notation}\label{table:notation}
		\begin{tabular}{|l|p{12cm}|}\hline
			Symbol & Meaning \\ \hline
			$C$&Any additive constant arising during the calculation of $\ELBO$. It can always be discarded, since we are interested in the values of parameters that maximize $\ELBO$, not the value of $\ELBO$. In some places I have used $C^\prime$, $C^{\prime\prime}$, etc, to indicate different additive constants.\\ \hline
			$\Expectation_q$& Expectation using distribution $q$ \\ \hline
			\glssymbol{gls:ELBOterm}&\glsdesc{gls:ELBOterm}\\ \hline
			$p(\vec{z},\vec{x})$ &Joint density of Observations, $\vec{x}$ and latent variables $\vec{z}$\\ \hline
			$q(\vec{z})$ &Any density chosen from $\mathcal{Q}$, the family that we use to model $p$\\ \hline
			$\vec{x}$&Observations \\ \hline
			$\vec{z}$ &Latent variables that (are intended to) explain Observations\\ \hline
		\end{tabular}
	\end{center}
\end{table}

\begin{thm}Bayes Theorem \cite{fisz1963probability}
	\begin{align*}
		p(\vec{z}\mid \vec{x}) =& \frac{p(\vec{x}, \vec{z})}{p(\vec{x})} \\
		=& \frac{\overbrace{p(\vec{x}| \vec{z})}^\text{Likelihood}}{\underbrace{p(\vec{x})}_\text{Evidence}} \overbrace{p(\vec{z})}^\text{Prior}
	\end{align*}
\end{thm}


The evidence is:
\begin{align*}
	p(z) =& \int p(x\mid z) p(z) dz \text{. We introduce the Kullback-Leibler divergence\cite{wiki:kullback:leibler}}\\
	\KLD{q(z)}{p(z\mid x)} \triangleq& \Expectation_q(\log{q(z)})- \Expectation_q(\log{p(z\mid x)}) \\
	=& \Expectation_q(\log{q(z)})- \Expectation_q(\log{p(z, x)}) + \Expectation_q(\log{p(x)})\\
	\Expectation_q(\log{p(x)})=&\KLD{q(z)}{p(z\mid x)}+\Expectation_q(\log{p(z, x)})-\Expectation_q(\log{q(z)})\\
	=& \KLD{q(z)}{p(z\mid x)} + \ELBO{(q(z))} \text{, where we define}\\
	\ELBO{(q)} \triangleq& \Expectation_q\big[\log{p(z,x)}\big] - \Expectation_q\big[\log{q(x)}\big]  \numberthis \label{eq:ELBO}
\end{align*}

Now we know that $\KLD{q(z)}{p(z\mid x)}\ge0$, with equality $iff\;q=p$, so:
\begin{align*}
	\Expectation_q(\log{p(x)})\ge\ELBO[q]
\end{align*}

The object of \gls{gls:VI} is to find a $q$ that matches $p$ by maximizing $L$.

\section{Coordinate Ascent Variational Inference}
In this section we derive the \gls{gls:ELBO} for \gls{gls:CAVI}. The following Lemma will be useful.

\begin{lemma}\label{lemma:gaussian}
	Moments of a Gaussian Distribution\cite[(5.7.5)]{fisz1963probability}.
	\begin{align*}
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} d \mu =& 1 \numberthis \label{eq:lemma:1}\\
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} \mu d \mu =& 0 \numberthis \label{eq:lemma:2}\\
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} \mu^2 d \mu =& \sigma^2 \numberthis \label{eq:lemma:3}
	\end{align*}
\end{lemma}

From \cite[Equation (13)]{blei2017variational}
\begin{align*}
	\ELBO{(q)} =& \Expectation_q\big[\log{p(\vec{z},\vec{x})}\big] - \Expectation_q\big[\log{q(\vec{x})}\big]  
\end{align*}

For a \gls{gls:GMM}--from \cite[Equations (7) \& (16)]{blei2017variational}
\begin{align*}
	q(\vec{\mu},\vec{c}) =& \prod_{k=1}^{K} q(\mu_k;m_k,s_k^2) \prod_{i=1}^{n}q(c_i;\phi_i) \numberthis \label{eq:gmm_q}\\
	p(\vec{\mu},\vec{c},\vec{x}) =& p(\vec{\mu}) \prod_{i=1}^{n} p(c_i) p(x_i \mid c_i,\vec{\mu}) \numberthis \label{eq:gmm_p}
\end{align*}

From \cite[Equation (21)]{blei2017variational}
\begin{align*}
	\ELBO(m,s^2,\phi) = \sum_{k=1}^{K}& \Expectation_q \big[ \log {p(\mu_k);m_k,s_k^2}\big]\\
	+& \sum_{k=1}^{K}\Expectation_q \big[\log{p(c_i);\phi_i}\big]\\
	 +& \sum_{k=1}^{K}\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big]\\
	-& \sum_{k=1}^K\Expectation_q\big[\log{q(c_i;\phi_i)}\big]\\
	 -& \sum_{k=1}^K\Expectation_q \big[\log{q(\mu_k;m_k,s^2_k)}\big] \numberthis \label{eq:ELBO_GMM}
\end{align*}
I shall expand these terms separately in equations  (\ref{eq:ELBO:1}), (\ref{eq:ELBO:2}), (\ref{eq:ELBO:3F}), (\ref{eq:ELBO:4}) and (\ref{eq:ELBO:5}).

From (\ref{eq:gmm_q}) and (\ref{eq:gmm_p})
\begin{align*}
	 \Expectation_q \big[ \log {p(\mu_k);m_k,s_k^2}\big]  =& \int_{-\infty}^{\infty} q(\mu_k;m_k,s_k^2) \log {p(\mu_k)}  d\mu_k\\
	 =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg]}\log{\bigg[\frac{1}{\sqrt{2\pi}} \exp{\big[- \frac{\mu_k^2}{2}\big]}\bigg]} d\mu_k\\
	 =&\frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg]}\bigg[ - \frac{\mu_k^2}{2}\bigg] d\mu_k+ C \text{, see Table \ref{table:notation} for $C$}\\
	 =& - \frac{1}{2 s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}(\mu_k+m_k)^2 d\mu_k \text{. Substitute $\mu_k \rightarrow \mu_k+m_k$}\\
	 =&- \frac{1}{s_k\sqrt{2\pi} 2 } \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}(\mu_k^2 +2 \mu_k m_k + m_k^2) d\mu_k\\
	 =&- \frac{1}{2 } \big(\underbrace{s_k^2}_\text{Lemma \ref{lemma:gaussian} \eqref{eq:lemma:3}} + \underbrace{0}_\text{Lemma \ref{lemma:gaussian} \eqref{eq:lemma:2}}+ \underbrace{m_k^2}_\text{Lemma \ref{lemma:gaussian} \eqref{eq:lemma:1}}\big)\\
	  =&- \frac{1}{2 } \big(s_k^2 +  m_k^2\big) \numberthis \label{eq:ELBO:1}
\end{align*}

\begin{align*}
	\Expectation_q\big[\log{p(c_i);\phi_i}\big] =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\big(\frac{(\mu_k-m_k)^2}{2 s_k^2}\big)\bigg]} \log{\frac{1}{K}} d \mu_k\\
	=& C  \text{, from Lemma \ref{lemma:gaussian}, where $C$ is defined in Table \ref{table:notation}.} \numberthis \label{eq:ELBO:2}
\end{align*}

From  (\ref{eq:gmm_p})
\begin{align*}
	\log{p(x_i\mid c_i,\mu)} =& \log{\big(\prod_{j=1}^{n} p(x_i|\mu_j)^{c_{ij}}\big)}\\
	=&\sum_{j=1}^{n}c_{ij} \log{p(x_i|\mu_j)}\\
	=& \log{\big(\prod_{j=1}^{n} p(x_i|\mu_j)^{c_{ij}}\big)}\\
	=&\sum_{j=1}^{n}c_{ij} \log{\frac{1}{\sqrt{2\pi}} \exp{\big[-\frac{(x_i-x_j)^2}{2}\big]}}
\end{align*}
Whence:
\begin{align*}
	\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big] =& - \frac{1}{2} c_{ij} \sum_{i=1}^{n} \sum_{j=1}^{n} \Expectation_q\bigg[ (x_i-\mu_j)^2\bigg] + C \numberthis \label{eq:ELBO:3}
\end{align*}

I will calculate each term within the summation separately.  

\begin{align*}
	\Expectation_q\big[(x_i-\mu_j)^2\big]
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg]}(x_i-\mu_k)^2 d \mu_k + C\\
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}[x_i-(\mu_k+m_k)]^2 d \mu_k + C^\prime \\
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}[(x_i-m_k)-\mu_k)]^2 d \mu_k\\
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}\big[(x_i-m_k)^2-2(x_i-m_k)\mu_k+\mu_k^2\big] d \mu_k\\
	=& (x_i-m_k)^2 + s_k^2 \text{, from Lemma \ref{lemma:gaussian}, so  (\ref{eq:ELBO:3}) yields}\\
	\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big] =&  -\frac{1}{2} \sum_{i=k}^{n} c_{ik} \big[s_k^2 + (x_k-m_k)^2 \big] \numberthis \label{eq:ELBO:3F}
\end{align*}
$\prod_{k=1}^{K} q(\mu_k;m_k,s_k^2) \prod_{i=1}^{n}q(c_i;\phi_i)$
\begin{align*}
	\Expectation_q\big[\log{q(c_i;\phi_i)}\big] =& \prod_{i=1}^{n}q(c_i;\phi_i) \big[\log{q(c_i;\phi_i)}\big]\\
	=& C \numberthis \label{eq:ELBO:4}
\end{align*}

\begin{align*} 
 	\Expectation \big[\log{q(\mu_k;m_k,s^2_k)}\big] =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg)} \bigg[\log{q(\mu_k;m_k,s^2_k)} \bigg] d\mu_k \\
 	 =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg)} \bigg[- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg] d\mu_k + C\\
 	  =& - \frac{1 }{2 s_k^2} \frac{1}{ s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k)^2}{2 s_k^2}\bigg)} \mu_k^2 d\mu_k + C\\
 	=& - \frac{s_k^2 }{2 s_k^2} \\
 	=& C \numberthis \label{eq:ELBO:5}
\end{align*}

We now substitute (\ref{eq:ELBO:1}), (\ref{eq:ELBO:2}), (\ref{eq:ELBO:3F}), (\ref{eq:ELBO:4}) and (\ref{eq:ELBO:5}) in (\ref{eq:ELBO_GMM}) 

\begin{align*}
	\ELBO(m,s^2,\phi) =& - \frac{1}{2 } \sum_{k=1}^{K} \bigg[ \big[s_k^2 +  m_k^2\big]+ m_k +  c_{ik} \big[s_k^2 + (x_k+m_k)^2 \big]\bigg] \numberthis \label{eq:ELBO:CAVI}
\end{align*}
% glossary : may need command makeglossaries.exe CAVI
\printglossaries

% bibliography

\bibliographystyle{unsrt}
\addcontentsline{toc}{section}{Bibliography}
\bibliography{learn}

\end{document}
