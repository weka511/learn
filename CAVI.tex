%    Copyright (C) 2020-2021 Greenweaves Software Limited

%   This program is free software: you can redistribute it and/or modify
%   it under the terms of the GNU General Public License as published by
%   the Free Software Foundation, either version 3 of the License, or
%   (at your option) any later version.

%  This program is distributed in the hope that it will be useful,
%  but WITHOUT ANY WARRANTY; without even the implied warranty of
%  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%  GNU General Public License for more details.

%  You should have received a copy of the GNU General Public License
%  along with this program.  If not, see <https://www.gnu.org/licenses/>.

\documentclass[]{article}

\usepackage{amsmath,amssymb,amsfonts,amsthm,cancel,float,mathrsfs,thmtools,url}
\usepackage[toc,page]{appendix}
\usepackage[toc,acronym,nonumberlist]{glossaries}

\newcommand{\ELBO[1]}        {\mathscr{L}}
\newcommand{\Expectation} {\mathbb{E}}
\newcommand{\KLD}[2]{D_{\mathrm{KL}} \left( \left. \left. #1 \right|\right| #2 \right) }
\newcommand\numberthis    {\addtocounter{equation}{1}\tag{\theequation}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}[thm]{Definition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}

%opening
\title{Coordinate Ascent Variational Inference}
\author{Simon Crase}

\makeglossaries

\newacronym{gls:CAVI}{CAVI} {Coordinate Ascent Variational Inference}
\newacronym{gls:ELBO}{ELBO} {Evidence Lower Bound}
\newacronym{gls:GMM} {GMM}  {Gaussian Mixture Model}
\newacronym{gls:VI}  {VI}   {Variational Inference}

\newglossaryentry{gls:ELBOterm}{
	name={Evidence Lower Bound},
	description={Evidence Lower Bound \eqref{eq:ELBO}, a lower bound on the probability of observing some data under a model},
	symbol={$\ELBO$}
}

\begin{document}

\maketitle

\begin{abstract}
	This document aims to fill the gaps between published papers, such as \cite{blei2017variational}, and my code. It has been written to help my understanding. If it is helpful to anyone else it is a bonus.
\end{abstract}

\tableofcontents
\listoftables
\listoftheorems

\section{Preliminaries}

\begin{defn}[the Kullback-Leibler divergence]
	Following \cite{wiki:kullback:leibler} we define the Kullback-Leibler divergence
	\begin{align*}
		\KLD{q(z)}{p(z)} \triangleq& \Expectation_q(\log{\frac{q(z)}{p(z)}}) 
	\end{align*}
\end{defn}

\begin{defn}[convex function]
	Let $f$ be a real valued function defined on [a,b]. $f$ is said to be convex iff $\forall x_1,x_2 \in [a,b]$ and $\forall \lambda \in [0,1]$
	\begin{align*}
		f(\lambda x_1 + (1-\lambda) x_2) \le \lambda f(x_1) + (1-\lambda)f(x_2) \numberthis \label{eq:convex}
	\end{align*}
\end{defn}

\begin{lemma}[Convexity]\label{lemma:convex}
	If $f^{\prime\prime}$ exists on $[a,b]$ and $f^{\prime\prime}\ge0$ on $[a,b]$, then $f$ is convex on $[a,b]$. 
\end{lemma}

\begin{lemma}[Jensen's inequality]\label{lemma:jensen}
	Let $f$ be a convex function defined on I. If $x_i \in I$ and $\lambda_i \ge 0$ and $\sum_{i=1}^{N} \lambda_i=1$. Then, for any $N>0$
	\begin{align*}
		f\big(\sum_{i=1}^{N} \lambda_i x_i\big) \le \sum_{i=1}^{N} \lambda_i f(x_i) \numberthis \label{eq:jensen}
	\end{align*}
\end{lemma}

\begin{proof}
	If $N=1$ the result is trivial, and for $N=2$ \eqref{eq:jensen} follows from \eqref{eq:convex}. We therefore assume that the result is true for some $N\ge2$, and seek to prove that it is true for $N+1$.
	\begin{align*}
		\sum_{i=1}^{N+1}\lambda_i f(x_i) =& \sum_{i=1}^{N}\lambda_i f(x_i) + \lambda_{N+1} f(x_{N+1})\\
		=& \sum_{i=1}^{N}\lambda_i f(x_i) + \lambda_{N+1} f(x_{N+1})\\
		=& \lambda \sum_{i=1}^{N}\lambda^\prime_i f(x_i) + (1-\lambda) f(x_{N+1}) \text{, where}\\
		\lambda =& \sum_{i=1}^{N}\lambda_i \text{, and}\\
		\lambda^\prime_i =& \frac{\lambda_i}{\lambda}
	\end{align*}
	Whence
	\begin{align*}
		\sum_{i=1}^{N+1}\lambda_i f(x_i) =& \lambda \sum_{i=1}^{N}\lambda^\prime_i f(x_i) + (1-\lambda) f(x_{N+1}) \\
		\ge& \lambda f\big(\sum_{i=1}^{N} \lambda^{\prime}_i x_i\big) + (1-\lambda) f(x_{N+1}) \text{, from \eqref{eq:convex}}\\
		\ge & f\big(\lambda \sum_{i=1}^{N} \lambda^{\prime}_i x_i + (1-\lambda) x_{N+1}\big) \text{, using \eqref{eq:jensen} as the induction hypothesis} \numberthis \label{eq:jensen:iteration}
	\end{align*} 
	But
	\begin{align*}
		\lambda \sum_{i=1}^{N} \lambda^{\prime}_i x_i + (1-\lambda) x_{N+1} =& \sum_{i=1}^{N} \lambda_i x_i + \lambda_{N+1}x_{N+1} \\
		=& \sum_{i=1}^{N+1} \lambda_i x_i
	\end{align*}
	So \eqref{eq:jensen:iteration} becomes
	\begin{align*}
		\sum_{i=1}^{N+1}\lambda_i f(x_i) \ge& f\big(\sum_{i=1}^{N+1} \lambda_i x_i\big)
	\end{align*}
\end{proof}


\begin{thm}[The Kullback-Leibler divergence is always non-negative.]\label{thm:kl:ge}
	\begin{align*}
		\KLD{q(z)}{p(z)} \ge 0 \numberthis \label{eq:kl:nn}
	\end{align*}
\end{thm}
\begin{proof}
	\begin{align*}
		\KLD{q(z)}{p(z)} = & \Expectation_q(\log{\frac{q(z)}{p(z)}})\\
        = & \Expectation_q(-\log{\frac{p(z)}{q(z)}})\\
         \ge &\log{\Expectation_q(\frac{p(z)}{q(z)})} \numberthis \label{eq:kl:nn0}
	\end{align*}
	 Where we have used  Jensen's inequality--Lemma \ref{lemma:jensen}--and the convexity of $-\log(x)$. The latter follows from Lemma \ref{lemma:convex} and the fact that $\frac{d^2\big(-\log{x}\big)}{dx^2}=\frac{1}{x^2}>0$. 
	\begin{align*}
		\log{\Expectation_q(\frac{p(z)}{q(z)})} =& \log{\sum_{i=0}^{N}\cancel{q_i}\frac{p_i}{\cancel{q_i}}}\\
		=& \log{\sum_{i=0}^{N}p_i}\\
		=& \log{1}\\
		=& 0
	\end{align*}
   Substituting in \eqref{eq:kl:nn0}, \eqref{eq:kl:nn} follows immediately.
\end{proof}

\section{Evidence Lower Bound}

\begin{table}[H]
	\begin{center}
		\caption{Notation used in this document}\label{table:notation}
		\begin{tabular}{|l|p{12cm}|}\hline
			Symbol & Meaning \\ \hline
			$C$&An additive constant arising during the calculation of $\ELBO$. It can always be discarded, since we are interested in the values of parameters that maximize $\ELBO$, not the value of $\ELBO$. In some places I have used $C^\prime$, $C^{\prime\prime}$, etc, to indicate different additive constants.\\ \hline
			$\sim$ & We write $a\sim b$ iff $a-b$ is independent of the latent variables $\vec{z}$.\\ \hline
			$\Expectation_q$& Expectation using distribution $q$ \\ \hline
			\glssymbol{gls:ELBOterm}&\glsdesc{gls:ELBOterm}\\ \hline
			$p(\vec{z},\vec{x})$ &Joint density of Observations, $\vec{x}$ and latent variables $\vec{z}$\\ \hline
			$K$&Number of Gaussians in \gls{gls:GMM}--dimension of $\vec{z}$\\ \hline
			$n$&Number of observations--dimension of $\vec{x}$\\ \hline
			$q(\vec{z})$ &Any density chosen from $\mathcal{Q}$, the family that we use to model $p$\\ \hline
			$\vec{x}$&Observations \\ \hline
			$\vec{z}$ &Latent variables that (are intended to) explain Observations\\ \hline
		\end{tabular}
	\end{center}
\end{table}

\begin{thm}[Bayes Theorem] \cite{fisz1963probability}
	\begin{align*}
		p(\vec{z}\mid \vec{x}) =& \frac{p(\vec{x}, \vec{z})}{p(\vec{x})} \\
		=& \frac{\overbrace{p(\vec{x}| \vec{z})}^\text{Likelihood}}{\underbrace{p(\vec{x})}_\text{Evidence}} \overbrace{p(\vec{z})}^\text{Prior}
	\end{align*}
\end{thm}


The evidence is:
\begin{align*}
		p(z) =& \int p(x\mid z) p(z) dz
\end{align*}
We introduce the Kullback-Leibler divergence\cite{wiki:kullback:leibler}:
\begin{align*}
\KLD{q(z)}{p(z\mid x)} \triangleq& \Expectation_q(\log{q(z)})-\Expectation_q(\log{p(z\mid x)}) \\
	=& \Expectation_q(\log{q(z)})- \Expectation_q(\log{p(z, x)}) + \Expectation_q(\log{p(x)})
\end{align*}
\begin{align*}
	\Expectation_q(\log{p(x)})=&\KLD{q(z)}{p(z\mid x)}+\Expectation_q(\log{p(z, x)})-\Expectation_q(\log{q(z)})\\
	=& \KLD{q(z)}{p(z\mid x)} + \ELBO{(q(z))} \numberthis \label{eq:ELBO:KL}
\end{align*}
Where  the \gls{gls:ELBO} $\ELBO$ is defined as:
\begin{align*}
	\ELBO{(q)} \triangleq& \Expectation_q\big[\log{p(z,x)}\big] - \Expectation_q\big[\log{q(x)}\big]  \numberthis \label{eq:ELBO}
\end{align*}
From Theorem \ref{thm:kl:ge}, $\KLD{q(z)}{p(z\mid x)}\ge0$, so \eqref{eq:ELBO:KL} gives:
\begin{align*}
	\Expectation_q(\log{p(x)})\ge\ELBO[q]
\end{align*}
\gls{gls:VI} aims to find a $q$ that matches $p$ by maximizing $\ELBO$.

\section{Coordinate Ascent Variational Inference}
I will derive the \gls{gls:ELBO} for \gls{gls:CAVI}. 

From \cite[Equation (13)]{blei2017variational}
\begin{align*}
	\ELBO{(q)} =& \Expectation_q\big[\log{p(\vec{z},\vec{x})}\big] - \Expectation_q\big[\log{q(\vec{x})}\big]  
\end{align*}

For a \gls{gls:GMM}--from \cite[Equations (7) \& (16)]{blei2017variational}
\begin{align*}
	q(\vec{\mu},\vec{c}) =& \prod_{k=1}^{K} q(\mu_k;m_k,s_k^2) \prod_{i=1}^{n}q(c_i;\phi_i) \numberthis \label{eq:gmm_q}\\
	p(\vec{\mu},\vec{c},\vec{x}) =& p(\vec{\mu}) \prod_{i=1}^{n} p(c_i) p(x_i \mid c_i,\vec{\mu}) \numberthis \label{eq:gmm_p}
\end{align*}


From \cite[Equation (21)]{blei2017variational}
\begin{align*}
	\ELBO(m,s^2,\phi) = \sum_{k=1}^{K}& \Expectation_q \big[ \log {p(\mu_k);m_k,s_k^2}\big]\\
	+& \sum_{i=1}^{n}\Expectation_q \big[\log{p(c_i);\phi_i}\big]\\
	 +& \sum_{i=1}^{n}\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big]\\
	-& \sum_{i=1}^n\Expectation_q\big[\log{q(c_i;\phi_i)}\big]\\
	 -& \sum_{k=1}^K\Expectation_q \big[\log{q(\mu_k;m_k,s^2_k)}\big] \numberthis \label{eq:ELBO_GMM}
\end{align*}
I shall expand these terms separately in equations  (\ref{eq:ELBO:1}), (\ref{eq:ELBO:2}), (\ref{eq:ELBO:3F}), (\ref{eq:ELBO:4}) and (\ref{eq:ELBO:5}), using the following Lemma.

\begin{lemma}[Moments of a Gaussian Distribution]\label{lemma:gaussian}
	\cite[(5.7.5)]{fisz1963probability}.
	\begin{align*}
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} d \mu =& 1 \numberthis \label{eq:lemma:1}\\
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} \mu d \mu =& 0 \numberthis \label{eq:lemma:2}\\
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} \mu^2 d \mu =& \sigma^2 \numberthis \label{eq:lemma:3}
	\end{align*}
\end{lemma}

From (\ref{eq:gmm_q}) and (\ref{eq:gmm_p})
\begin{align*}
	 \Expectation_q \big[ \log {p(\mu_k);m_k,s_k^2}\big]  =& \int_{-\infty}^{\infty} q(\mu_k;m_k,s_k^2) \log {p(\mu_k)}  d\mu_k\\
	 =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\big[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\big]}\log{\big[\frac{1}{\sqrt{2\pi}} \exp{\big[- \frac{\mu_k^2}{2}\big]}\big]} d\mu_k\\
	 =&\frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\big[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\big]}\big[ - \frac{\mu_k^2}{2}\big] d\mu_k+ C \text{, see Table \ref{table:notation} for $C$}\\
	 =& - \frac{1}{2 s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]}(\mu_k+m_k)^2 d\mu_k \text{. Substitute $\mu_k \rightarrow \mu_k+m_k$}\\
	 =&- \frac{1}{2}\frac{1}{s_k\sqrt{2\pi} } \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]}(\mu_k^2 +2 \mu_k m_k + m_k^2) d\mu_k\\
	 =&- \frac{1}{2}\bigg[\underbrace{\frac{1}{s_k\sqrt{2\pi} } \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]}\mu_k^2  d\mu_k}_\text{$=s_k^2$--Lemma \ref{lemma:gaussian} \eqref{eq:lemma:3}}\\
	 \;&+2  m_k \underbrace{\frac{1}{s_k\sqrt{2\pi} } \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]} \mu_k  d\mu_k}_\text{$=0$--Lemma \ref{lemma:gaussian} \eqref{eq:lemma:2}}\\
	 \;&+m_k^2 \underbrace{\frac{1}{s_k\sqrt{2\pi} } \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]} d\mu_k}_\text{$=1$--Lemma \ref{lemma:gaussian} \eqref{eq:lemma:1}}\bigg]\\
	  =&- \frac{1}{2 } \big(s_k^2 +  m_k^2\big) \numberthis \label{eq:ELBO:1}
\end{align*}

\begin{align*}
	\Expectation_q\big[\log{p(c_i);\phi_i}\big] =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\big(\frac{(\mu_k-m_k)^2}{2 s_k^2}\big)\bigg]} \log{\frac{1}{K}} d \mu_k\\
	=& \log{\frac{1}{K}} \underbrace{\frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}  d \mu_k}_\text{$=1$--from Lemma \ref{lemma:gaussian}, \eqref{eq:lemma:1}}\\
	\sim & 0 \numberthis \label{eq:ELBO:2}
\end{align*}

From  (\ref{eq:gmm_p})
\begin{align*}
	\log{p(x_i\mid c_i,\mu)} =& \log{\big(\prod_{j=1}^{n} p(x_i|\mu_j)^{c_{ij}}\big)}\\
	=&\sum_{j=1}^{n}c_{ij} \log{p(x_i|\mu_j)}\\
	=& \log{\big(\prod_{j=1}^{n} p(x_i|\mu_j)^{c_{ij}}\big)}\\
	=&\sum_{j=1}^{n}c_{ij} \log{\frac{1}{\sqrt{2\pi}} \exp{\big[-\frac{(x_i-x_j)^2}{2}\big]}}
\end{align*}
Whence:
\begin{align*}
	\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big] =& - \frac{1}{2} c_{ij} \sum_{i=1}^{n} \sum_{j=1}^{n} \Expectation_q\bigg[ (x_i-\mu_j)^2\bigg] + C \numberthis \label{eq:ELBO:3}
\end{align*}

I will calculate each term within the summation separately.  

\begin{align*}
	\Expectation_q\big[(x_i-\mu_j)^2\big]
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg]}(x_i-\mu_k)^2 d \mu_k + C\\
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}[x_i-(\mu_k+m_k)]^2 d \mu_k + C^\prime \\
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}[(x_i-m_k)-\mu_k)]^2 d \mu_k\\
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}\big[(x_i-m_k)^2-2(x_i-m_k)\mu_k+\mu_k^2\big] d \mu_k\\
	=& (x_i-m_k)^2 + s_k^2 \text{, from Lemma \ref{lemma:gaussian}, so  (\ref{eq:ELBO:3}) yields}\\
	\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big] =&  -\frac{1}{2} \sum_{i=k}^{n} c_{ik} \big[s_k^2 + (x_k-m_k)^2 \big] \numberthis \label{eq:ELBO:3F}
\end{align*}
$\prod_{k=1}^{K} q(\mu_k;m_k,s_k^2) \prod_{i=1}^{n}q(c_i;\phi_i)$
\begin{align*}
	\Expectation_q\big[\log{q(c_i;\phi_i)}\big] =& \prod_{i=1}^{n}q(c_i;\phi_i) \big[\log{q(c_i;\phi_i)}\big]\\
	=& C \numberthis \label{eq:ELBO:4}
\end{align*}

\begin{align*} 
 	\Expectation \big[\log{q(\mu_k;m_k,s^2_k)}\big] =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg)} \bigg[\log{q(\mu_k;m_k,s^2_k)} \bigg] d\mu_k \\
 	 =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg)} \bigg[- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg] d\mu_k + C\\
 	  =& - \frac{1 }{2 s_k^2} \frac{1}{ s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k)^2}{2 s_k^2}\bigg)} \mu_k^2 d\mu_k + C\\
 	=& - \frac{s_k^2 }{2 s_k^2} \\
 	=& C \numberthis \label{eq:ELBO:5}
\end{align*}

So \eqref{eq:ELBO_GMM} becomes: 

\begin{align*}
	\ELBO(m,s^2,\phi) = & \underbrace{\sum_{k=1}^{K} \Expectation_q \big[ \log {p(\mu_k);m_k,s_k^2}\big]}_\text{$=- \frac{1}{2 } \big(s_k^2 +  m_k^2\big)$ from \eqref{eq:ELBO:1}}\\
	&+ \underbrace{\sum_{i=1}^{n}\Expectation_q \big[\log{p(c_i);\phi_i}\big]}_\text{$=0$ from \eqref{eq:ELBO:2}}\\
	&+ \underbrace{\sum_{i=1}^{n}\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big]}_\text{$=-\frac{1}{2} \sum_{i=k}^{n} c_{ik} \big[s_k^2 + (x_k-m_k)^2 \big]$ from \eqref{eq:ELBO:3F}}\\
	&+ \underbrace{\sum_{i=1}^n\Expectation_q\big[\log{q(c_i;\phi_i)}\big]}_\text{$=0$ from \eqref{eq:ELBO:4}}\\
	&+ \underbrace{\sum_{k=1}^K\Expectation_q \big[\log{q(\mu_k;m_k,s^2_k)}\big]}_\text{$=0$ from \eqref{eq:ELBO:5}} \\
 	=& - \frac{1}{2 } \sum_{k=1}^{K} \bigg[ s_k^2 +  m_k^2 + m_k +  c_{ik} \big[s_k^2 + (x_k+m_k)^2 \big]\bigg] \numberthis \label{eq:ELBO:CAVI}
\end{align*}


% glossary : may need command makeglossaries.exe CAVI
\printglossaries

% bibliography

\bibliographystyle{unsrt}
\addcontentsline{toc}{section}{Bibliography}
\bibliography{learn}

\end{document}
