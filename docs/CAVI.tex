%    Copyright (C) 2020-2025 Greenweaves Software Limited

%   This program is free software: you can redistribute it and/or modify
%   it under the terms of the GNU General Public License as published by
%   the Free Software Foundation, either version 3 of the License, or
%   (at your option) any later version.

%  This program is distributed in the hope that it will be useful,
%  but WITHOUT ANY WARRANTY; without even the implied warranty of
%  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
%  GNU General Public License for more details.

%  You should have received a copy of the GNU General Public License
%  along with this program.  If not, see <https://www.gnu.org/licenses/>.

\documentclass[]{article}

\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{cancel}
\usepackage{float}
\usepackage{mathrsfs}
\usepackage{thmtools}
\usepackage{url}
\usepackage[toc,page]{appendix}
\usepackage[toc,acronym,nonumberlist]{glossaries}

\newcommand{\ELBO[1]} {\mathscr{L}}
\newcommand{\Expectation} {\mathbb{E}}
\newcommand{\KLD}[2]{D_{\mathrm{KL}} \left( \left. \left. #1 \right|\right| #2 \right) }
\newcommand\numberthis {\addtocounter{equation}{1}\tag{\theequation}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}[thm]{Definition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}

%opening
\title{Coordinate Ascent Variational Inference}
\author{Simon Crase}

\makeglossaries

\newacronym{gls:CAVI}{CAVI} {Coordinate Ascent Variational Inference}
\newacronym{gls:ELBO}{ELBO} {Evidence Lower Bound}
\newacronym{gls:GMM} {GMM}  {Gaussian Mixture Model}
\newacronym{gls:VI}  {VI}   {Variational Inference}

\newglossaryentry{gls:ELBOterm}{
	name={Evidence Lower Bound},
	description={$\ELBO$--\eqref{eq:ELBO}}-- provides a lower bound on the probability of observing some data under a model
}

\newglossaryentry{gls:KL}{
	name={Kullback-Leibler Divergence},
	description={$\KLD{q(z)}{p(z)}$--\eqref{eq:kl}--measures the information lost by representing the true probability distribution $p(x)$ by $q(x)$}
}

\begin{document}

\maketitle

\begin{abstract}
	This document aims to fill the gaps between published papers and my code in order to help my understanding. If it is helpful to anyone else it is a bonus.
	
	Section \ref{sect:kl} describes the Kullback-Leibler divergence. Section \ref{sect:elbo} introduces the \gls{gls:ELBO}, and Section \ref{sect:cavi} computes the \gls{gls:ELBO} for a \gls{gls:GMM}.
\end{abstract}

\tableofcontents
\listoftables
\listoftheorems

\section{Kullback-Leibler divergence}\label{sect:kl}

Following \cite{blei2017variational} I define the \gls{gls:KL}, and derive Theorem \ref{thm:kl:ge}, which will be used in the sequel. \glsdesc{gls:KL}.
	
\begin{defn}[Kullback-Leibler divergence]
	\begin{align*}
		\KLD{q(z)}{p(z)} \triangleq& \Expectation_q(\log{\frac{q(z)}{p(z)}}) \numberthis \label{eq:kl}
	\end{align*}
\end{defn}

\begin{defn}[Convex function]
	A real valued function defined on [a,b], $f$, is said to be convex iff $\forall x_1,x_2 \in [a,b]$ and $\forall \lambda \in [0,1]$
	\begin{align*}
		f(\lambda x_1 + (1-\lambda) x_2) \le \lambda f(x_1) + (1-\lambda)f(x_2) \numberthis \label{eq:convex}
	\end{align*}
\end{defn}

\begin{lemma}[Test for Convexity]\label{lemma:convex}
	If $f^{\prime\prime}$ exists on $[a,b]$ and $f^{\prime\prime}\ge0$ on $[a,b]$, then $f$ is convex on $[a,b]$. 
\end{lemma}

\begin{lemma}[Jensen's inequality]\label{lemma:jensen}
	Let $f$ be a convex function defined in some interval $I$. If $x_i \in I$ and $\lambda_i \ge 0$ and $\sum_{i=1}^{N} \lambda_i=1$. Then, for any $N>0$
	\begin{align*}
		f\big(\sum_{i=1}^{N} \lambda_i x_i\big) \le \sum_{i=1}^{N} \lambda_i f(x_i) \numberthis \label{eq:jensen}
	\end{align*}
\end{lemma}

\begin{proof}
	If $N=1$ the result is trivial, and if $N=2$, \eqref{eq:jensen} follows from \eqref{eq:convex}. We therefore assume that the result is true for some $N\ge2$, and seek to prove that it is true for $N+1$.
	\begin{align*}
		\sum_{i=1}^{N+1}\lambda_i f(x_i) =& \sum_{i=1}^{N}\lambda_i f(x_i) + \lambda_{N+1} f(x_{N+1})\\
		=& \lambda \sum_{i=1}^{N}\lambda^\prime_i f(x_i) + (1-\lambda) f(x_{N+1}) \text{, where}\\
		\lambda =& \sum_{i=1}^{N}\lambda_i \text{, and}\\
		\lambda^\prime_i =& \frac{\lambda_i}{\lambda}
	\end{align*}
	Whence
	\begin{align*}
		\sum_{i=1}^{N+1}\lambda_i f(x_i) =& \lambda \sum_{i=1}^{N}\lambda^\prime_i f(x_i) + (1-\lambda) f(x_{N+1}) \\
		\ge& \lambda f\big(\sum_{i=1}^{N} \lambda^{\prime}_i x_i\big) + (1-\lambda) f(x_{N+1}) \text{, from \eqref{eq:convex}}\\
		\ge & f\big(\lambda \sum_{i=1}^{N} \lambda^{\prime}_i x_i + (1-\lambda) x_{N+1}\big) \text{, by the induction hypothesis}. \numberthis \label{eq:jensen:iteration}
	\end{align*} 
	Now
	\begin{align*}
		\lambda \sum_{i=1}^{N} \lambda^{\prime}_i x_i + (1-\lambda) x_{N+1} =& \sum_{i=1}^{N} \lambda_i x_i + \lambda_{N+1}x_{N+1} \\
		=& \sum_{i=1}^{N+1} \lambda_i x_i
	\end{align*}
	So \eqref{eq:jensen:iteration} becomes
	\begin{align*}
		\sum_{i=1}^{N+1}\lambda_i f(x_i) \ge& f\big(\sum_{i=1}^{N+1} \lambda_i x_i\big)
	\end{align*}
\end{proof}


\begin{thm}[The \gls{gls:KL} is always non-negative]\label{thm:kl:ge}
	\begin{align*}
		\KLD{q(z)}{p(z)} \ge 0 \numberthis \label{eq:kl:nn}
	\end{align*}
\end{thm}
\begin{proof}
	From \text{\eqref{eq:kl}}
	\begin{align*}
		\KLD{q(z)}{p(z)} = & \Expectation_q(\log{\frac{q(z)}{p(z)}})\\
        = & \Expectation_q(-\log{\frac{p(z)}{q(z)}})
	\end{align*}
	Using Lemma \ref{lemma:convex} and the fact that $\frac{d^2\big(-\log{x}\big)}{dx^2}=\frac{1}{x^2}>0$, we see that $-\log(x)$ is convex. We can therefore apply Jensen's inequality--Lemma \ref{lemma:jensen}.
	\begin{align*}
		\KLD{q(z)}{p(z)} \ge &-\log{\Expectation_q(\frac{p(z)}{q(z)})} \numberthis \label{eq:kl:nn0}
	\end{align*}
	Now
	\begin{align*}
		\log{\Expectation_q(\frac{p(z)}{q(z)})} =& \log{\sum_{i=1}^{N}\cancel{q_i}\frac{p_i}{\cancel{q_i}}}\\
		=& \log{\sum_{i=1}^{N}p_i}\\
		=& \log{1}\\
		=& 0
	\end{align*}
   So \eqref{eq:kl:nn0} reduces to \eqref{eq:kl:nn}.
\end{proof}

\section{Evidence Lower Bound}\label{sect:elbo}

\begin{table}[H]
	\begin{center}
		\caption[Notation used in this document]{Notation used in this document. Note that the true probabilities, $p$ and $\mathcal{P}$, are hidden, so we will need to model them using $q$ and $\mathcal{Q}$.}\label{table:notation}
		\begin{tabular}{|l|p{12cm}|}\hline
			Symbol & Meaning \\ \hline
			$C$&Any additive constant arising during the calculation of $\ELBO$. It can always be discarded, since we are interested in the values of parameters that maximize $\ELBO$, not the value of $\ELBO$. In some places I have used $C^\prime$, $C^{\prime\prime}$, etc, to indicate different additive constants.\\ \hline
			$\sim$ & We write $a\sim b$ iff $a-b$ is independent of the latent variables $\vec{z}$.\\ \hline
			$\Expectation_q$& Expectation using distribution $q$ \\ \hline
			\glssymbol{gls:ELBOterm}&\glsdesc{gls:ELBOterm}\\ \hline
			$p(\vec{z},\vec{x})$ &Joint density of Observations, $\vec{x}$ and latent variables $\vec{z}$.\\ \hline
			$\mathcal{P}$&Family of distributions for $p(\vec{z},\vec{x})$\\ \hline
			$K$&Number of Gaussians in \gls{gls:GMM}--dimension of $\vec{z}$\\ \hline
			$n$&Number of observations--dimension of $\vec{x}$\\ \hline
			$\mathcal{Q}$&A family of distributions that we use to model reality\\ \hline
			$q(\vec{z})$ &Any density chosen from $\mathcal{Q}$, the family that we use to model $p$\\ \hline
			$\vec{x}$&Observations \\ \hline
			$\vec{z}$ &Latent variables that (are intended to) explain Observations\\ \hline
		\end{tabular}
	\end{center}
\end{table}

\begin{thm}[Bayes Theorem] \cite{fisz1963probability}
	\begin{align*}
		p(\vec{z}\mid \vec{x}) =& \frac{p(\vec{x}, \vec{z})}{p(\vec{x})} \\
		=& \frac{\overbrace{p(\vec{x}| \vec{z})}^\text{Likelihood}}{\underbrace{p(\vec{x})}_\text{Evidence}} \overbrace{p(\vec{z})}^\text{Prior}
	\end{align*}
\end{thm}

The evidence is:
\begin{align*}
		p(z) =& \int p(x\mid z) p(z) dz
\end{align*}
Since this is generally intractable\cite{blei2017variational}, we will use the \gls{gls:KL} to derive a bound.
\begin{align*}
\KLD{q(z)}{p(z\mid x)} \triangleq& \Expectation_q(\log{q(z)})-\Expectation_q(\log{p(z\mid x)}) \\
	=& \Expectation_q(\log{q(z)})- \Expectation_q(\log{p(z, x)}) + \Expectation_q(\log{p(x)}) \text{, rearranging}\\
	\Expectation_q(\log{p(x)})=&\KLD{q(z)}{p(z\mid x)}+\Expectation_q(\log{p(z, x)})-\Expectation_q(\log{q(z)})\\
	=& \KLD{q(z)}{p(z\mid x)} + \ELBO{(q(z))} \numberthis \label{eq:ELBO:KL}
\end{align*}
Where $\ELBO$ is defined to be:
\begin{align*}
	\ELBO{(q)} \triangleq& \Expectation_q\big[\log{p(z,x)}\big] - \Expectation_q\big[\log{q(x)}\big]  \numberthis \label{eq:ELBO}
\end{align*}
See \cite[Equation (13)]{blei2017variational}.
From Theorem \ref{thm:kl:ge}, $\KLD{q(z)}{p(z\mid x)}\ge0$, so \eqref{eq:ELBO:KL} gives:
\begin{align*}
	\Expectation_q(\log{p(x)})\ge\ELBO[q]
\end{align*}
This explains the name \glsdesc{gls:ELBO}.
\gls{gls:VI} aims to find a $q\in\mathcal{Q}$ that matches $p$ as well as $\mathcal{Q}$ permits, by maximizing $\ELBO$.

\section{Coordinate Ascent Variational Inference}\label{sect:cavi}
I will derive the \gls{gls:ELBO} for \gls{gls:CAVI}.

For a \gls{gls:GMM}--from \cite[Equations (7) \& (16)]{blei2017variational} with $K$ Gaussians and a sample size $n$:
\begin{align*}
	q(\vec{\mu},\vec{c}) =& \prod_{k=1}^{K} q(\mu_k;m_k,s_k^2) \prod_{i=1}^{n}q(c_i;\phi_i) \numberthis \label{eq:gmm_q}\\
	p(\vec{\mu},\vec{c},\vec{x}) =& p(\vec{\mu}) \prod_{i=1}^{n} p(c_i) p(x_i \mid c_i,\vec{\mu}) \numberthis \label{eq:gmm_p}
\end{align*}


Substituting  \eqref{eq:gmm_p} and \eqref{eq:gmm_q} in \eqref{eq:ELBO} we get \cite[Equation (21)]{blei2017variational}
\begin{align*}
	\ELBO(m,s^2,\phi) = \sum_{k=1}^{K}& \Expectation_q \big[ \log {p(\mu_k);m_k,s_k^2}\big]\\
	+& \sum_{i=1}^{n}\Expectation_q \big[\log{p(c_i);\phi_i}\big]\\
	 +& \sum_{i=1}^{n}\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big]\\
	-& \sum_{i=1}^n\Expectation_q\big[\log{q(c_i;\phi_i)}\big]\\
	 -& \sum_{k=1}^K\Expectation_q \big[\log{q(\mu_k;m_k,s^2_k)}\big] \numberthis \label{eq:ELBO_GMM}
\end{align*}
I shall expand these terms separately in equations  \eqref{eq:ELBO:1}, \eqref{eq:ELBO:2}, \eqref{eq:ELBO:3F}, \eqref{eq:ELBO:4}, and \eqref{eq:ELBO:5}, using Lemma \ref{lemma:gaussian} of Appendix \ref{section:appendix}.


From (\ref{eq:gmm_q}) and (\ref{eq:gmm_p})
\begin{align*}
	 \Expectation_q \big[ \log {p(\mu_k);m_k,s_k^2}\big]  =& \int_{-\infty}^{\infty} q(\mu_k;m_k,s_k^2) \log {p(\mu_k)}  d\mu_k\\
	 =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\big[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\big]}\log{\big[\frac{1}{\sqrt{2\pi}} \exp{\big[- \frac{\mu_k^2}{2}\big]}\big]} d\mu_k\\
	 =&\frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\big[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\big]}\big[ - \frac{\mu_k^2}{2}\big] d\mu_k+ C \text{, see Table \ref{table:notation} for $C$}\\
	 =& - \frac{1}{2 s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]}(\mu_k+m_k)^2 d\mu_k \text{. Substitute $\mu_k \rightarrow \mu_k+m_k$}\\
	 =&- \frac{1}{2}\frac{1}{s_k\sqrt{2\pi} } \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]}(\mu_k^2 +2 \mu_k m_k + m_k^2) d\mu_k\\
	 =&- \frac{1}{2}\bigg[\underbrace{\frac{1}{s_k\sqrt{2\pi} } \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]}\mu_k^2  d\mu_k}_\text{$=s_k^2$--Lemma \ref{lemma:gaussian} \eqref{eq:lemma:3}}\\
	 \;&+2  m_k \underbrace{\frac{1}{s_k\sqrt{2\pi} } \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]} \mu_k  d\mu_k}_\text{$=0$--Lemma \ref{lemma:gaussian} \eqref{eq:lemma:2}}\\
	 \;&+m_k^2 \underbrace{\frac{1}{s_k\sqrt{2\pi} } \int_{-\infty}^{\infty} \exp{\big[-\frac{\mu_k^2}{2 s_k^2}\big]} d\mu_k}_\text{$=1$--Lemma \ref{lemma:gaussian} \eqref{eq:lemma:1}}\bigg]\\
	  =&- \frac{1}{2 } \big(s_k^2 +  m_k^2\big) \numberthis \label{eq:ELBO:1}
\end{align*}

\begin{align*}
	\Expectation_q\big[\log{p(c_i);\phi_i}\big] =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\big(\frac{(\mu_k-m_k)^2}{2 s_k^2}\big)\bigg]} \log{\frac{1}{K}} d \mu_k\\
	=& \log{\frac{1}{K}} \underbrace{\frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}  d \mu_k}_\text{$=1$--from Lemma \ref{lemma:gaussian}, \eqref{eq:lemma:1}}\\
	\sim & 0 \text{, where $\sim$ is defined in Table \ref{table:notation}}\numberthis \label{eq:ELBO:2}
\end{align*}

From  (\ref{eq:gmm_p}), and using $c_{ik}$ to denote the $k$th component of $c_i$,
\begin{align*}
	\log{p(x_i\mid c_i,\mu)} =& \log{\big(\prod_{k=1}^{K} p(x_i|\mu_k)^{c_{ik}}\big)}\\
	=&\sum_{k=1}^{K}c_{ik} \log{p(x_i|\mu_k)}\\
	=&\sum_{k=1}^{n}c_{ik} \log{\frac{1}{\sqrt{2\pi}} \exp{\big[-\frac{(x_i-\mu_k)^2}{2}\big]}} \text{, whence}\\
	\sum_{i=1}^{n}\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big] =& - \frac{1}{2}  \sum_{i=1}^{n} \sum_{k=1}^{K} c_{ik} \Expectation_q\bigg[ (x_i-\mu_k)^2\bigg] + C  \numberthis \label{eq:ELBO:3}
\end{align*}

I will calculate each term within the summation separately.  

\begin{align*}
	\Expectation_q\big[(x_i-\mu_k)^2\big]
	=& \prod_{k^\prime=1}^{K}\frac{1}{s_{k^\prime}\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{(\mu_{k^\prime}-m_{k^\prime})^2}{2 s_{k^\prime}^2}\bigg]}(x_i-\mu_k)^2 d \mu_{k^\prime}\\
	=& \frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg]}(x_i-\mu_k)^2 d \mu_k
\end{align*}
since each integral in the product evaluates to $1$ except when ${k^\prime}=k$. Substituting $\mu^\prime_k=\mu_k-m_k$
\begin{align*}
	\Expectation_q\big[(x_i-\mu_k)^2\big]=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{{\mu^\prime_k}^2}{2 s_k^2}\bigg]}[x_i-(\mu^\prime_k+m_k)]^2 d \mu^\prime_k + C^\prime \\
	\sim&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}[(x_i-m_k)-\mu_k)]^2 d \mu_k\\
	=&\frac{1}{s_k\sqrt{2\pi}}  \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu_k^2}{2 s_k^2}\bigg]}\big[(x_i-m_k)^2-2(x_i-m_k)\mu_k+\mu_k^2\big] d \mu_k\\
	=& (x_i-m_k)^2 + s_k^2 \text{, from Lemma \ref{lemma:gaussian}, so  (\ref{eq:ELBO:3}) yields}\\
	\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big] =&  -\frac{1}{2} \sum_{i=1}^{n} \sum_{k=1}^{K}c_{ik} \big[s_k^2 + (x_i-m_k)^2 \big] \numberthis \label{eq:ELBO:3F}
\end{align*}
$\prod_{k=1}^{K} q(\mu_k;m_k,s_k^2) \prod_{i=1}^{n}q(c_i;\phi_i)$
\begin{align*}
	\Expectation_q\big[\log{q(c_i;\phi_i)}\big] =& \prod_{i=1}^{n}q(c_i;\phi_i) \big[\log{q(c_i;\phi_i)}\big]\\
	=& C \numberthis \label{eq:ELBO:4}
\end{align*}

\begin{align*} 
 	\Expectation \big[\log{q(\mu_k;m_k,s^2_k)}\big] =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg)} \bigg[\log{q(\mu_k;m_k,s^2_k)} \bigg] d\mu_k \\
 	 =& \frac{1}{s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg)} \bigg[- \frac{(\mu_k-m_k)^2}{2 s_k^2}\bigg] d\mu_k + C\\
 	  =& - \frac{1 }{2 s_k^2} \frac{1}{ s_k\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg(- \frac{(\mu_k)^2}{2 s_k^2}\bigg)} \mu_k^2 d\mu_k + C\\
 	=& - \frac{s_k^2 }{2 s_k^2} \\
 	=& C \numberthis \label{eq:ELBO:5}
\end{align*}

So \eqref{eq:ELBO_GMM} becomes: 

\begin{align*}
	\ELBO(m,s^2,\phi) = & \underbrace{\sum_{k=1}^{K} \Expectation_q \big[ \log {p(\mu_k);m_k,s_k^2}\big]}_\text{$=- \frac{1}{2 } \sum_{k=1}^{K} \big(s_k^2 +  m_k^2\big)$ from \eqref{eq:ELBO:1}}\\
	&+ \underbrace{\sum_{i=1}^{n}\Expectation_q \big[\log{p(c_i);\phi_i}\big]}_\text{$=0$ from \eqref{eq:ELBO:2}}\\
	&+ \underbrace{\sum_{i=1}^{n}\Expectation_q\big[\log{p(x_i\mid c_i,\mu);\phi_i,m,s^2}\big]}_\text{$= -\frac{1}{2} \sum_{i=1}^{n} \sum_{k=1}^{K}c_{ij} \big[s_k^2 + (x_i-m_k)^2 \big]$ from \eqref{eq:ELBO:3F}}\\
	&+ \underbrace{\sum_{i=1}^n\Expectation_q\big[\log{q(c_i;\phi_i)}\big]}_\text{$=0$ from \eqref{eq:ELBO:4}}\\
	&+ \underbrace{\sum_{k=1}^K\Expectation_q \big[\log{q(\mu_k;m_k,s^2_k)}\big]}_\text{$=0$ from \eqref{eq:ELBO:5}} \\
	=&- \frac{1}{2 } \sum_{k=1}^{K} \big(s_k^2 +  m_k^2\big) -\frac{1}{2} \sum_{i=1}^{n} \sum_{k=1}^{K}c_{ik} \big[s_k^2 + (x_i-m_k)^2 \big] \numberthis \label{eq:ELBO:CAVI}
\end{align*}
See also \cite{blei2011variational}

\appendix 

\section{Moments of the Gaussian Distribution}\label{section:appendix}

\begin{lemma}[Moments of the Gaussian Distribution]\label{lemma:gaussian}
	\begin{align*}
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} d \mu =& 1 \numberthis \label{eq:lemma:1}\\
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} \mu d \mu =& 0 \numberthis \label{eq:lemma:2}\\
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} \mu^2 d \mu =& \sigma^2 \numberthis \label{eq:lemma:3}
	\end{align*}
\end{lemma}
\begin{proof}
	\begin{align*}
		\int_{-\infty}^{\infty} \exp{[-x^2]} d x 	\int_{-\infty}^{\infty} \exp{[-y^2]} d y=&\int_{x=-\infty}^{\infty} \int_{y=-\infty}^{\infty} \exp{\bigg[-(x^2+y^2)\bigg]} d x d y\\
		=&\int_{r=0}^{\infty} \int_{\theta=0}^{2\pi} exp{(-r^2}) r dr d\theta\\
		=&\int_{r=0}^{\infty}  exp{[-r^2]} r dr \int_{\theta=0}^{2\pi} d\theta\\
		=&\int_{r=0}^{\infty}  exp{[-r^2]} r dr 2\pi\\
		=&\frac{\cancel{2}\pi}{\cancel{2}}\int_{R=0}^{\infty}  exp{[-R]} dR \\
		=&\pi
	\end{align*}
	Substituting $x=\frac{\mu}{\sqrt{2}\sigma}$
	\begin{align*}
		\frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} d \mu =& \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} \exp{[-x^2]} d x\\
		=&\frac{\cancel{\pi}}{\cancel{\pi}}
	\end{align*}	
	Which reduces to \eqref{eq:lemma:1}. Note that \eqref{eq:lemma:2} is immediate, since the integrand is odd, and \eqref{eq:lemma:3} can be deduced by rearranging \eqref{eq:lemma:1} and differentiating w.r.t. $\sigma$.
	
	\begin{align*}
		\frac{d}{d\sigma} \int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} d \mu =& \frac{d}{d\sigma} \sigma\sqrt{2\pi}\\
		\int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} \frac{\cancel{(-2)}\mu^2}{\cancel{(-2)}\sigma^3} d \mu =& \sqrt{2\pi}\\
		\int_{-\infty}^{\infty} \exp{\bigg[-\frac{\mu^2}{2 \sigma^2}\bigg]} d \mu =&\sqrt{2\pi} \sigma^3 \text{, which rearranges to \eqref{eq:lemma:3}.}
	\end{align*}
\end{proof}

% glossary : may need command makeglossaries.exe CAVI
\printglossaries

% bibliography

\bibliographystyle{unsrt}
\addcontentsline{toc}{section}{Bibliography}
\bibliography{learn}

\end{document}
